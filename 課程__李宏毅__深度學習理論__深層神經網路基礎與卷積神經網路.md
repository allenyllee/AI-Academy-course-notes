# 課程__李宏毅__深度學習理論__深層神經網路基礎與卷積神經網路

###### tags: `課程` `深度學習理論`

:::info
編輯請按左上角的筆<i class="fa fa-pencil"></i>或旁邊的雙欄模式<i class="fa fa-columns"></i>。請以登入模式幫助編輯喔！
:::

課程投影片: https://drive.google.com/file/d/14LFJHwVRwzrSyhigFO-n_lbrCfQj4-JW/view

課程提問: https://app2.sli.do/event/oknxaime/ask

Gradient Descent Demo Code:
https://drive.google.com/file/d/1M7MOvwS0VWGVPC_vfr1ICNC7X-dr4lga/view

可參考筆記: 2016 一天搞懂深度學習心得筆記(ihower)
https://ihower.tw/blog/archives/8574

[toc]

# Day 01 - Feb 12

## Introduction

### Machine Learning $\approx$ Looking for a Function

Deep Learning 是 Machine Learning 的其中一種方法

- Speech Recognition (語音辨識)

    $$ f(\mathrm{聲音訊號}) = \mathrm{"How\ are\ you"} $$
        
    聲音訊號跟文字之間的關係是非常複雜的，就算是同一個人說的 How are you，也不會每次都產生同樣的聲音訊號，憑藉著人類的力量 (60 年代嘗試過了) 是很難找出這樣一個函數。所以就必須憑藉著機器的力量來幫我們找。
        
- Image Recognition (影像辨識)

    $$ f(\mathrm{貓的圖片}) = \mathrm{"Cat"} $$
        
- Playing Go (下圍棋)

    $$ f(\mathrm{棋盤的盤勢}) = \mathrm{"5-5" (下一步)} $$
        
- Dialogue System (對話系統)

    $$ f(\mathrm{使用者說的話}) = \mathrm{"系統的回應"} $$
        
### Three Steps for Deep Learning

   1. define a set of function (Model)

        神經網路的架構 (神經元的連接方式) 就決定了候選函數集
        
   2. evaluate goodness of function

        訓練資料 (input -> output) => Supervised Learning
        好壞的標準是由人來訂定的
        
   3. pick the best function

        需要有好的演算法

### Neural Network

神經網路的架構 (神經元的連接方式) 就決定了候選函數集
  
#### Neuron (神經元)

一個神經元就代表了一個簡單的函數 ($f: \mathbb{R}^K \rightarrow \mathbb{R}$)

$$ f(\vec{a}) = a = \sigma(\vec{w} \cdot \vec{a} + b) $$

其中 $\vec{w}$ 是 weights, $b$ 是 bias, $\sigma({\bullet})$ 是 activation function。

- $\vec{w}$ 和 $b$ 是由機器找出來的
- $\sigma({\bullet})$ 是由人設定的
    - 好壞判定尚無定論

1980 年代常用的一個 activation function 是 Sigmoid function，現在已經很少人用了

$$ \sigma(z) = \frac{1}{1 + e^{-z}} $$

一個神經元的輸出可以是其他神經元的輸入。

Diffent connections lead to different network structures (也就是不同的天賦)

The neurons have different values of weights and biases (機器自己後天學習的成果)

Weights and biases are called network parameters $\theta$。

#### Fully Connected Feedforward Network

將神經元分成幾組，每一組叫做一個 layer，每一個 layer 的神經元都跟下一個 layer 的神經元完全連接。

假設網路參數已經靠機器後天學習到了，給定輸入向量，根據神經元的計算公式，會將每一層的計算結果當作下一層的輸入，一層一層傳遞，直到抵達輸出層作為最後的結果。

Given parameters $\theta$, a function is defined.

$$ f_\theta(\vec{x}) = \vec{y} $$

Given netwrok structure, a <u>function set</u> is defined.

$$ \{ f_\theta: \mathbb{R}^N \Rightarrow \mathbb{R}^M | \forall \theta \} $$

由於計算過程並沒有隨機性，同樣一個輸入值，最後得到的輸出值會是一樣的，這也就符合了函數的特性。

一般來說，FCFN 會由下面的示意圖表示

$$ \vec{x} \Rightarrow Layer1 \Rightarrow Layer2 \Rightarrow \dots Layer L \Rightarrow \vec{y} $$

其中
- $\vec{x}$ 是 Ｎ 維向量，雖然它不包含神經元，但習慣上也還會把它稱之作 Input Layer。
- $\vec{y}$ 是 M 維向量，也就是 Layer L (Output Layer) 的輸出值。
- 不是輸入層也不是輸出層的 Layers 都稱作 Hidden Layers。
- Deep 指的就是很多 Hidden Layer。

### Matrix Operation

假如神經元的 weights 和 bias 已知，就可以使用矩陣運算來計算神經元的輸出。

$$ 
\sigma \left(
\underbrace{
\begin{bmatrix}
1 & -2 \\
-1 & 1
\end{bmatrix}
\begin{bmatrix}
1 \\ -1
\end{bmatrix} +
\begin{bmatrix}
1 \\ -1
\end{bmatrix}
}
\right) =
\begin{bmatrix}
0.98 \\ 0.12
\end{bmatrix}
$$

### Function of a Neural Network

將多層神經元的矩陣運算展開可以得到神經網路所代表的函數。

$$
y = f(x) = \sigma(W^L \dots \sigma(W^2 \sigma(W^1 x + b^1) + b^2) \dots + b^L)
$$

> 矩陣運算用 GPU 來做會比 CPU 快很多

### Deep = Many hidden layers

影像辨識的比賽

年份 | 網路名稱    | 網路層數 | 錯誤率
----|------------|:--------:|-------:
2012 | AlexNet | 8 | 16.4%
2014 | VGG | 19 | 7.3 %
2014 | GoogleNet | 22 | 6.7 %
2015 | Residual Net | 152 | 3.57 %

已經證實 3.57% 是比人類所達到的錯誤率還低，不過這是因為 Corpus 過於變態，需要正確辨識出動物的品種才算對 (e.g. 加州海獅、北海獅、...)

### Special Network Structures

Feedforward network 是這一層的輸出會是下一層的輸入，下一層的輸出又是在下一層的輸入，一層接著一層將計算結果傳遞下去。

當網路到達一定深度後，有時候會 train 不起來，也就是會找不到參數。

- Residual Network

    會在層與層之間加上跨越多層的捷徑，將前面某個 Layer 的輸出複製一份直接與這層的輸出加起來

- Highway Network

    在輸出和輸入之間加上一個 2-way 開關，用來決定下一層輸入要採用上一層的輸出或是更上一層的輸出。
    
    這個開關的控制需要講到 Gate 的概念，通常在講到 RNN 的時候，在講到 LSTM 的時候才會提到。
    
    基本概念就是機器會根據輸入來決定開關的方向。

    :::success
    採用 Highway Network 網路架構的好處是機器可以決定要使用的 Layer 個數。
    :::

    [THE MNIST DATABASE of handwritten digits](http://yann.lecun.com/exdb/mnist/)
    
    [CIFAR-10/CIFAR-100 Databases](https://www.cs.toronto.edu/~kriz/cifar.html)
    
### Output Layer

- Ordinary layer

    $$
    y_1 = \sigma(z_1)\\
    y_2 = \sigma(z_2)\\
    y_3 = \sigma(z_3)
    $$

    In general, the output of network can be any value.

    例如，LEMU 的輸出為 0 到 $\infty$

    這會不太容易解釋 network 輸出的結果，有一種常見的做法是讓 network 的輸出看起來像機率 (比較容易解釋)。

- Softmax layer as the output layer

    $$
    y_1 = \frac{e^{z_1}}{\sum_{j=1}^3 e^{z_j}}\\
    y_2 = \frac{e^{z_2}}{\sum_{j=1}^3 e^{z_j}}\\
    y_3 = \frac{e^{z_3}}{\sum_{j=1}^3 e^{z_j}}
    $$
    
### Example Application

手寫數字辨識

輸入：16x16 點陣圖 $\Rightarrow \begin{bmatrix} x_1 & x_2 & \dots & x_{256}\end{bmatrix}^T$
輸出：$\begin{bmatrix}y_1 & y_2 & \dots & y_{10}\end{bmatrix}^T$ (每一維度表示其對應數字的信心程度)

> 你今天必須要定出一個好的 function set，至少讓這個 function set 包含好的 function，最好還希望排除掉那些不好的 function，讓機器在這個 pool 裡面可以挑出一個最好的 function。
> 
> 那如果你今天 funtion set 定得不好的話，到時候你在找 function 的時候就會變成說，你想要大海撈針，結果那根針還不在海裡。
> [name=李宏毅]
 
### FAQ

Q: How many layers? How many neurons for each layer?

A: Trial and Error + Intuition

Q: Can we design the network structure?

A: E.g. Convolutional Neural Network (CNN)

Q: Can the structure be automatically determined?

A: Yes, but not widely used yet...

### Training Data

如果想要評估一個 function 的好壞，要先準備訓練資料，這些訓練資料就是一些 examples。

以手寫數字辨識為例，就會是一堆手寫數字圖片跟與其對應的數字標籤

### Learning Target

對於所有的訓練資料，神經網路的輸出都跟給定的標籤一致 (e.g. 圖片為 "1"，則 $y_1$ 為最大值)

### Loss

評量函數好壞的 function，用來計算神經網路的輸出與正確答案(target)之間的差距

可以是 **square error** or **cross entropy**

不同的 Loss ($l$) 對於訓練過程會有微妙的影響

### Total Loss

將所有訓練資料的 loss 總和起來，是真正拿來評估 function 好壞的量測標準，希望越小越好。

$$ L = \sum_{r=1}^R l_r $$
 
### How to pick the best function
  
Find network parameters $\theta^∗$ that minimize total loss $L$

最笨的想法是窮舉所有可能的 $\theta$，然後挑出會讓 total  loss 最小的 (但參數可能有上百萬，這麼搞是行不通的)

### Gradient Descent

假設參數只有 $w$，簡單說明梯度下滑的精神

- Pick an initial value for $w$ (Random)
- Compute $\frac{\partial L}{\partial w}$

    - Negative $\Rightarrow$ Increase $w$
    - Positive $\Rightarrow$ Decrease $w$

- Update $w$

    $$ w \leftarrow w - \eta \frac{\partial L}{\partial w} $$
    
    其中 $\eta$ 稱為 learning rate
    
- Repeat until $\partial L / \partial w \approx 0$ (when update is little)

---

假設參數 $\theta$ 一般化成一個向量

$$
\theta =
\begin{bmatrix}
w_1\\
w_2\\
\vdots\\
b_1\\
\vdots
\end{bmatrix}
$$

Gradient Descent 會變成

- Pick an initial $\theta$
- Compute $\nabla L$ (gradient)

    $$ 
    \nabla L = 
    \begin{bmatrix}
    \frac{\partial L}{\partial w_1}\\
    \frac{\partial L}{\partial w_2}\\
    \vdots\\
    \frac{\partial L}{\partial b_1}\\
    \vdots
    \end{bmatrix}
    $$
    
- Update $\theta$

    $$ \theta \leftarrow \theta - \eta \nabla L $$
    
### Local Minima

從山坡上的任一點出發，朝最陡峭的方向下坡，到了谷底，由於坡度已經為零，就不會再繼續走下去，但也許並非整體的最低點。

GD 也會在馬鞍點卡住
GD 在平原上會很慢

- GD never guarantee global minima

- Different initial point $\Rightarrow$ Reach different minima, so diffrent results

    > GD 是個看人品的方法
    
### DEMO: Age of Empires II

戰場上是有戰霧的，除了戰車所經之地，是看不到地圖上其他地方的 (除非用 marco 開天眼)，因此無法預先得知最低點的位置。

就算是最潮的 AlphaGo 其實也是用 GD 訓練的
AutoML 裡面雖然是用 RL 但是也是用 GD 訓練的

> 你以為 AI 就是這麼潮，其實跟打世紀帝國是一樣的啦
> 希望你不要覺得太失望
> [name=李宏毅]
    
### Backpropagation

GD 用在深度學習就叫做 Backpropagation

- Backpropagation: an efficient way to computer $\partial L / \partial w$ in neural network
- 多數的 Deep Learning 的 Framework 都支援計算 Backpropagation 的功能

- [三十分钟理解计算图上的微积分：Backpropagation，反向微分 - Bin 的专栏 - CSDN博客](http://blog.csdn.net/xbinworld/article/details/56523063)

## Why Deep?

 * why deep? Universality Theorem
::: info
**Universality Theorem**
  Any continuous function f, Can be realized by a network with one hidden layer.(given enough hidden neurons)
:::

 * Deeper is Better?
   1.more parameters, better performance.
   2.using more layer can saving parameters to  get good performance.
   3.Fat+Short v.s. Thin+Tall

 * Deep->Modularization->Less training data
  (the modularization is automatically learned from data)

 * Analogy: Logic circuits v.s. Neural network

 * End-to-end Learning
    What each function should do is learned automatically
  
 * Knowledge Distillation
  
  

## gradient descent
  * review gradient descent
  **Learning rate v.s. training Loss**
  ![](https://scontent.ftpe7-1.fna.fbcdn.net/v/t1.0-9/27858099_2164787266865558_1360925158538831647_n.jpg?_nc_fx=ftpe7-2&oh=0392c1bb8882eb35b5e873a81a770993&oe=5B208441)
  
  Adaptive Learning Rates
  :::info 
**Adagrad:**
  Divide the learning rate of each   parameter by the root mean square of its previous derivatives.
  :::
  
  * Stochastic Gradient Descent(SGD)
   make the training faster(v.s. Gradient Descent)

  * Mini-batch
  Smaller batch size means more updates in one epoch.

  * Matrix Operation(Using GPU speed up)

  * Feature Scaling(Normalization)

  * Gradient Descent Theory

[深度学习最全优化方法总结比较（SGD，Adagrad，Adadelta，Adam，Adamax，Nadam）](https://zhuanlan.zhihu.com/p/22252270)


## backpropagation
  * Gradient Descent
  * Chain Rule
  * Backpropagation-Forward pass
  * Backpropagation-Backward pass
  * Backpropagation-Summary


## DNN tip

## CNN

## Auto-encoder

## Q&A 

[2/12(一) 深度學習理論: 深層神經網路基礎與卷積神經網路 - sli.do](https://app2.sli.do/event/oknxaime/ask)

- 請問, 若有100個batch, 是每次隨機抽1個batch 取後放回 的方式嗎？還是從第1個慢慢跑到最後一個？ 請問一次epoch結束後, 要重新隨機設定每一個batch內的instance嗎？ 謝謝

    > 一個例子是，有個學生嘗試訓練一個網路，輸入影片輸出文字敘述，但performance 一直無法做到跟別人一樣好，找了很久都找不到原因。後來才發現，因為每段影片都有不同幾筆敘述，當做mini-batch 時，也要把這些敘述也做隨機分散。 [name=李宏毅]

- 請問老師，Knowledge Distillation訓練出的shallow network在實務上有應用的價值嗎?似乎accuracy和參數量都沒有比較好和少

    > 使用較淺的網路進行模仿訓練，可以學到深度網路的performance，因為層數較少，而且同一層內的計算都可平行化，可以減少運算時間，很適合放入硬體資源受限的嵌入式系統。 [name=李宏毅]


- 牛頓法是讓gradient descent往等於0走, 但gradient descent = 0 不見得是loss小的地方, 例如: saddle point 梯度為0，但不是最大或最小的值。
