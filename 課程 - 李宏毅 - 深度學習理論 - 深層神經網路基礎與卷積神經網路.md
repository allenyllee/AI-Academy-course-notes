# 課程 - 李宏毅 - 深度學習理論 - 深層神經網路基礎與卷積神經網路

:::info
編輯請按左上角的筆<i class="fa fa-pencil"></i>或旁邊的雙欄模式<i class="fa fa-columns"></i>。請以登入模式幫助編輯喔！
:::

課程投影片: https://drive.google.com/file/d/14LFJHwVRwzrSyhigFO-n_lbrCfQj4-JW/view

課程提問: https://app2.sli.do/event/oknxaime/ask

Gradient Descent Demo Code:
https://drive.google.com/file/d/1M7MOvwS0VWGVPC_vfr1ICNC7X-dr4lga/view

可參考筆記: 2016 一天搞懂深度學習心得筆記(ihower)
https://ihower.tw/blog/archives/8574

[toc]

## Introduction

### Machine Learning $\approx$ Looking for a Function

Deep Learning 是 Machine Learning 的其中一種方法

- Speech Recognition (語音辨識)

    $$ f(\mathrm{\text{聲音訊號}}) = \mathrm{"How\ are\ you"} $$
        
    聲音訊號跟文字之間的關係是非常複雜的，就算是同一個人說的 How are you，也不會每次都產生同樣的聲音訊號，憑藉著人類的力量 (60 年代嘗試過了) 是很難找出這樣一個函數。所以就必須憑藉著機器的力量來幫我們找。
        
- Image Recognition (影像辨識)

    $$ f(\mathrm{\text{貓的圖片}}) = \mathrm{"Cat"} $$
        
- Playing Go (下圍棋)

    $$ f(\mathrm{\text{棋盤的盤勢}}) = \mathrm{\text{"5-5" (下一步)}} $$
        
- Dialogue System (對話系統)

    $$ f(\mathrm{\text{使用者說的話}}) = \mathrm{\text{"系統的回應"}} $$
        
### Three Steps for Deep Learning:

   1. define a set of function (Model)

        神經網路的架構 (神經元的連接方式) 就決定了候選函數集
        
   2. evaluate goodness of function

        訓練資料 (input -> output) => Supervised Learning
        好壞的標準是由人來訂定的
        
   3. pick the best function

        需要有好的演算法

### Neural Network

神經網路的架構 (神經元的連接方式) 就決定了候選函數集
  
#### Neuron (神經元)

一個神經元就代表了一個簡單的函數 ($f: \mathbb{R}^K \rightarrow \mathbb{R}$)

$$ f(\vec{a}) = a = \sigma(\vec{w} \cdot \vec{a} + b) $$

其中 $\vec{w}$ 是 weights, $b$ 是 bias, $\sigma({\bullet})$ 是 activation function。

- $\vec{w}$ 和 $b$ 是由機器找出來的
- $\sigma({\bullet})$ 是由人設定的
    - 好壞判定尚無定論

1980 年代常用的一個 activation function 是 Sigmoid function，現在已經很少人用了

$$ \sigma(z) = \frac{1}{1 + e^{-z}} $$

一個神經元的輸出可以是其他神經元的輸入。

Diffent connections lead to different network structures (也就是不同的天賦)

The neurons have different values of weights and biases (機器自己後天學習的成果)

Weights and biases are called network parameters $\theta$。

### Fully Connected Feedforward Network

將神經元分成幾組，每一組叫做一個 layer，每一個 layer 的神經元都跟下一個 layer 的神經元完全連接。

假設網路參數已經靠機器後天學習到了，給定輸入向量，根據神經元的計算公式，會將每一層的計算結果當作下一層的輸入，一層一層傳遞，直到抵達輸出層作為最後的結果。

Given parameters $\theta$, a function is defined.

$$ f_\theta(\vec{x}) = \vec{y} $$

Given netwrok structure, a <u>function set</u> is defined.

$$ \{ f_\theta: \mathbb{R}^N \Rightarrow \mathbb{R}^M | \forall \theta \} $$

由於計算過程並沒有隨機性，同樣一個輸入值，最後得到的輸出值會是一樣的。另外一個輸入值，也會得到一個輸出值。

一般來說，FCFN 會由下面的示意圖表示

$$ \vec{x} \Rightarrow Layer1 \Rightarrow Layer2 \Rightarrow \ldots Layer L => \vec{y} $$

其中
- $\vec{x}$ 是 Ｎ 維向量雖然不包含神經元，但習慣上也還會把它稱之作 Input Layer。
- $\vec{y}$ 是 M 維向量，也就是 Layer L (Output Layer) 的輸出值。
- 不是輸入層也不是輸出層的 Layers 都稱作 Hidden Layers。



  Matrix Operation
  Deep = Many hidden layers
     Residual Network
     Highway Network

 * Output Layer
     Softmax layer as the output layer

 * Training Data
  Learning Target
  Loss: (square error or cross entropy)
  Total Loss
 
 * How to pick the best function
  Find network parameters θ^∗ that minimize total loss L

 * Gradient Descent
    Local Minima
 
 * Backpropagation
   * [三十分钟理解计算图上的微积分：Backpropagation，反向微分 - Bin 的专栏 - CSDN博客](http://blog.csdn.net/xbinworld/article/details/56523063)

## why deep?
 * why deep? Universality Theorem
::: info
**Universality Theorem**
  Any continuous function f, Can be realized by a network with one hidden layer.(given enough hidden neurons)
:::

 * Deeper is Better?
   1.more parameters, better performance.
   2.using more layer can saving parameters to  get good performance.
   3.Fat+Short v.s. Thin+Tall

 * Deep->Modularization->Less training data
  (the modularization is automatically learned from data)

 * Analogy: Logic circuits v.s. Neural network

 * End-to-end Learning
    What each function should do is learned automatically
  
 * Knowledge Distillation
  
  

## gradient descent
  * review gradient descent
  **Learning rate v.s. training Loss**
  ![](https://scontent.ftpe7-1.fna.fbcdn.net/v/t1.0-9/27858099_2164787266865558_1360925158538831647_n.jpg?_nc_fx=ftpe7-2&oh=0392c1bb8882eb35b5e873a81a770993&oe=5B208441)
  
  Adaptive Learning Rates
  :::info 
**Adagrad:**
  Divide the learning rate of each   parameter by the root mean square of its previous derivatives.
  :::
  
  * Stochastic Gradient Descent(SGD)
   make the training faster(v.s. Gradient Descent)

  * Mini-batch
  Smaller batch size means more updates in one epoch.

  * Matrix Operation(Using GPU speed up)

  * Feature Scaling(Normalization)

  * Gradient Descent Theory


## backpropagation
  * Gradient Descent
  * Chain Rule
  * Backpropagation-Forward pass
  * Backpropagation-Backward pass
  * Backpropagation-Summary


## DNN tip

## CNN

## Auto-encoder

## Q&A 

[2/12(一) 深度學習理論: 深層神經網路基礎與卷積神經網路 - sli.do](https://app2.sli.do/event/oknxaime/ask)

- 請問, 若有100個batch, 是每次隨機抽1個batch 取後放回 的方式嗎？還是從第1個慢慢跑到最後一個？ 請問一次epoch結束後, 要重新隨機設定每一個batch內的instance嗎？ 謝謝

    > 一個例子是，有個學生嘗試訓練一個網路，輸入影片輸出文字敘述，但performance 一直無法做到跟別人一樣好，找了很久都找不到原因。後來才發現，因為每段影片都有不同幾筆敘述，當做mini-batch 時，也要把這些敘述也做隨機分散。 [name=李宏毅]

- 請問老師，Knowledge Distillation訓練出的shallow network在實務上有應用的價值嗎?似乎accuracy和參數量都沒有比較好和少

    > 使用較淺的網路進行模仿訓練，可以學到深度網路的performance，因為層數較少，而且同一層內的計算都可平行化，可以減少運算時間，很適合放入硬體資源受限的嵌入式系統。 [name=李宏毅]


- 牛頓法是讓gradient descent往等於0走, 但gradient descent = 0 不見得是loss小的地方, 例如: saddle point 梯度為0，但不是最大或最小的值。
