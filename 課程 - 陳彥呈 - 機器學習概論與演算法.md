# 課程 - 陳彥呈 - 機器學習概論與演算法

###### tags: `ai-academy` `ML`

回首頁 [AI Academy 上課筆記 - HackMD](https://hackmd.io/MbAs4TlAGBaBGAHAIwIa1KgTAM1qpCWYCHANgGYBWM6Adhy0SA==?view)

:::info
編輯請按左上角的筆<i class="fa fa-pencil"></i>或旁邊的雙欄模式<i class="fa fa-columns"></i>。請以登入模式幫助編輯喔！
:::

[toc]

## 主題：    
    
* ML 
* Image Processing

## Type of Learning

* 監督式學習（Supervised Learning）: 在訓練的過程中告訴機器答案、也就是「有標籤」的資料，比如給機器各看了 1000 張蘋果和橘子的照片後、詢問機器新的一張照片中是蘋果還是橘子。
* 非監督式學習（Unsupervised Learning）: 訓練資料沒有標準答案、不需要事先以人力輸入標籤，故機器在學習時並不知道其分類結果是否正確。訓練時僅須對機器提供輸入範例，它會自動從這些範例中找出潛在的規則。
* Reinforcement Learning
    * Q Policy
    * Reward
* Evolutionary Learning
    * Fitness

## 專有名詞
* (O) 閾(ㄩˋ)值
* (X) 閥(ㄈㄚˊ)值

## Accuracy Metrics
### Accuracy
$$ \frac{True\;Positive\;\;|\;False\;Positive}{\;False\;Negative\;|\;True\;Negative\;} $$
$$ Accuracy = \frac{\#TP+\#FP}{\#TP+\#FP+TN+FN}$$
### Precision and Recall
$$ Precision = \frac{\#TP}{\#TP+\#FP} $$
$$ Recall = \frac{\#TP}{\#TP+\#FN} $$
### F1 Score
$$F_1 = 2* \frac{precision\;*\;recall}{precision\;+\;recall}$$ 
$$= \frac{\#TP}{\#TP+(\#FN+\#FP)/2}$$
* (#FP+#FN)/2 = Mean of false samples
### Matthew's Correlation Coefficient
* 如果 sample size 有 unbalanced 的現象可以利用此 accuacy metric 去測量預測精準度
$$MCC= \frac{TP*TN - FP*FN}{\sqrt{(TP+FP)*(TP+FN)*(TN+FP)*(TN+FN)}}$$
## Hierarchical Clustering

### Agglomerative Clustering

## DBSCAN

* limitation
    * $\epsilon$ 不好選
    * 分群的效果與 $\epsilon$ 的選擇有關，如果出現大範圍比較鬆散的結構，DBSCAN 很難將之分出來

## Spectral Clustering
* [譜分群 (Spectral Clustering) ─ 運用圖論 (Graph Theory) 進行分群 – David's Perspective](https://taweihuang.hpd.io/2017/07/06/intro-spectral-clustering/)
* [漫谈 Clustering (4): Spectral Clustering « Free Mind](http://blog.pluskid.org/?p=287)


### Noramlized Cut
$$ \mathrm{Noramlized \; Cut}(A,B) = \frac{\mathrm{cut}(A,B)}{\mathrm{volume}(A)}+\frac{\mathrm{cut}(A,B)}{\mathrm{volume}(B)}$$

## Image Processing

* [Local Historgram Equalization](http://scikit-image.org/docs/dev/auto_examples/color_exposure/plot_local_equalize.html#sphx-glr-auto-examples-color-exposure-plot-local-equalize-py): Help normalize image before send to ML

# Supervised Learning
## Classifier

### Naive Bayes Classifer
$$ P(X|Y) = \frac{p(X|Y)p(Y)}{p(X)}, \; where \; p(X) = \sum_{Y}{p(X|Y)p(Y)}$$

assume:

$$ p(X_1 ... X_n|Y) = \prod_{_i}{p(X_i|Y)}$$

Decision rule:
$$ Y^* = \mathop{\arg\max}_{Y} {\;}{p(Y)p(X_1, ..., X_n|Y)}$$
$$      = \mathop{\arg\max}_{Y} {\;}{p(Y)\prod_{i}{p(X_i|Y)}} $$

* limitation
  * NB分類結果的特性通常是接近0或接近1，如果想要做多標籤分類比較困難


* 語音識別的問題：
    1. 即使在背景乾淨的情況下，也很難到達98%
    2. 背景噪音很多時，準確率會下降很多
    3. 對腔調很敏感，腔調差一點就無法辨識
    4. 多人同時講話時，就無法辨識

### Logistic Regression
* Learn $p(Y|X)$ directly, $y$-intercept define probability
$$ 1 < \frac{P(Y = 1|X)}{P(Y=0|X)} $$
* Objective Function for Logistic Regression is **Cross Entropy**
$$\max L(\theta) = \max \sum_{i=1}^N y_i \log \hat{y}_i + (1-y_i) \log (1- \hat{y}_i)$$

* Naive v.s. Logistic comparison (圖)
http://papers.nips.cc/paper/2020-on-discriminative-vs-generative-classifiers-a-comparison-of-logistic-regression-and-naive-bayes.pdf

* 補充log likelihood function of logistic regression推導:
(感謝at071045同學分享)
https://stats.stackexchange.com/questions/235514/how-do-i-get-cost-function-of-logistic-regression-in-scikit-learn-from-log-likel
![](https://scontent.ftpe8-1.fna.fbcdn.net/v/t1.0-9/27540822_2158191720858446_9012023248352962375_n.jpg?oh=b78ef29f59bbe715743a7e775710a031&oe=5B1D74B5)

### Perceptron
* Simplest NN
* Single Layer
* Cannot solve XOR problem
* 林軒田（圖）p.30 decision boundary
https://www.csie.ntu.edu.tw/~htlin/talk/doc/mltour.handout.pdf
* 林軒田 機器學習公開課 [Learning to Answer Yes/No :: Perceptron Learning Algorithm @ Machine Learning Foundations (機器學習基石) - YouTube](https://www.youtube.com/watch?v=1xnUlrgJJGo&index=7&list=PLXVfgk9fNX2I7tB6oIINGBmW50rrmFTqf)

### Linear SVM

* Use Max Margin

* svm理論 canonical hyperplane
https://mattermost.aiacademy.tw/api/v4/files/4zgzajzmx7yjujo8w4zu6seoka

* CSE446: SVMs Spring 2017 
[8_SVMs - 8_SVMs.pdf](https://courses.cs.washington.edu/courses/cse446/17sp/Slides/8_SVMs.pdf)

### Decision Tree / Forest
* Information Gain visualized http://www.math.unipd.it/~aiolli/corsi/0708/IR/Lez12.pdf
* random forest example (p.111)
https://www.csie.ntu.edu.tw/~htlin/talk/doc/mltour.handout.pdf

## Meta Learning
* meta learning
  * [最前沿：百家争鸣的Meta Learning/Learning to learn](https://zhuanlan.zhihu.com/p/28639662)

#### Ensembles
一個decision tree 效果不好，但多個decision tree 結合可以得到較好的效能

* Methods
    * Bootstrap
    * Boosting
    * Adaboost
    * XGBoost

#### Semi-Supervised Learning (SSL)
* 對於假設定義很敏感，錯誤的假設會造成分類錯誤。


# Computer Vision 經驗分享
講義：https://www.slideshare.net/albertycchen/practical-computer-vision-a-realworld-problemdriven-approach-to-learning-cvmldl

## SIFT
* [OpenCV SIFT](https://docs.opencv.org/3.1.0/da/df5/tutorial_py_sift_intro.html)
## Talks:
eg. Industrial inspection: 沒有用到ML，只是image processing。先做一個3D模型，然後把label做座標轉換。

eg. License Plate Recognition: 目前台灣的沒有用ML。把車牌投影到x軸y軸，切出來。

eg. Automated Fingerprint Identification

eg. Face Recognition用adaboast / haar features. 頭分上半下半部，另外一半塗黑。

eg. Nokia想做智慧相冊，自動標籤，但準確率只有三成。換個方式包裝。sharpness 自動對焦，改善手晃，

eg. NFL自動劃線，sensor自動會有球場的線，有不同的視角。可能也沒有用到image processing，computer vision。

eg. 3D Reconstruction: as old as CV; became practical since SIFT.

eg. Image Panoramas (1980-now): 不是難在相片怎麼對映。難是難在把圖接起來。Markov Random Field。把prior定成edge的information function。另解：用大量data學。2000年，Belief propagation改善很多，決定要用哪一張照片的顏色值。

eg. Solving photosynthesis problems with alpha matting.

eg. Object detection and classification 與自動駕駛有關。一開始先去辨識路上的路障，但以前深度學習不普及，辨識率低。2007 DARPA Urban challenge＠內華達州城市挑戰。Nvidia self driving car 2016，沒有特別的sensor，只有接方向盤和油門的output，不再盯著障礙物（inspired by 賽車手）。並不是一開始就狂掃路上的障礙物。

eg. Structure from X PIX4D。空拍機建構3D模型

eg. Object Recognition Blue River Technology。辨識雜草或紅蘿波。可放在農耕機後面。可用到有機農耕。看到是雜草灑熱水。

eg. False positive需要人力去辨別。醫學影像需要克服是否能同時增加accuracy&recall

eg. Application in Business Intelligence。

eg. AOI breakthrough with DL-Metal Inspection。抓出瑕疵。只用五層NN，最後是fully connected。影像是32ppixel，所以不需要太複雜的模型。

eg. Laser Welding雷射電焊。

eg. Serial Number Processing把過去百年的生產儀器，全部電子化。不是一個簡單的問題，點陣圖。

eg. 皮膚疾病檢測。

eg. 醫療影像，傳統方法有困難(hard feature正方形長方形)，需要DL。

eg. plant disease recognition。
    
eg. Smart Surveillance。智慧型監控，台灣約有三十家在做。有電信商想做熟客分析，熱點偵測，客服服務時間，顧客滿意度。做AI垂直領域。也有公司，專門先把人detect出來，在完美的segmentation出來，剩下來的先不管。


eg. 智慧型零售。Viscovery，手機拍攝可以兌換，但市場沒有準備好。但有特定族群eg. 女裝。不一定會找到一模一樣的東西。又或者是美妝品，鉑來品難以輸入外語，照片幫助很大。每天需要更新的商品比例：20% SKU update/per day，upload to google?類似人臉辨識，賣關子，看距離。商品分成四五百個小類，各自的距離（尺）是可以接受的。

eg. 中國傾全力發展 face++ 孫劍 商湯科技 Multiview Camera。很多個鏡頭拍到的是否是同一個人。

eg. 自動駕駛台灣的競爭力。目前各家廠商都在passenger car，但是用到別的領域，或許還有機會eg 礦區。DJI(大江)。
















# Dataset
  [UCI Machine Learning Repository: Data Sets](https://archive.ics.uci.edu/ml/datasets.html)



# ML推薦書籍
* [Bishop: Pattern Recognition And Machine Learning](http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf)
* [Richard O Duda: Pattern.Classification](http://cns-classes.bu.edu/cn550/Readings/duda-etal-00.pdf)
* [Ian Goodfellow: Deep Learning](http://www.deeplearningbook.org/) 
* [華盛頓大學 - 機器學習](https://courses.cs.washington.edu/courses/cse446/)


## 延伸閱讀資料
* [【最优化】为什么最速下降法中迭代方向是锯齿形的？](https://zhuanlan.zhihu.com/p/33428233)
* 

# Q&A
- [machine learning 1 - slido](https://app.sli.do/event/bijqisrj/ask)

Q: 對於機器學習來說，收集到樣本的features彼此越獨立學習效果越好嗎?
A: empirical study 發現越獨立越好。

Q: 老師所介紹的clustering methodologies是否全部一定都會收斂? 各個clustering 的methodology的收斂速度為何 (big O) ? 如何評估clustering的結果好壞?
A: 有可能不收斂。收斂速度待查。

Q: 在使用 Clustering 演算法時，如果沒有 Label 的資料，要如何評估分群結果的好壞呢
A: 會有leave out data(不叫做training/validation/test data)。有無quantify的方法，待查。




## 備註

數學式子打法

$$
\mathrm{cov}(x, y) = \frac{1}{n-1} \sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})
$$



